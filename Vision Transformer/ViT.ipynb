{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df357946",
   "metadata": {},
   "source": [
    "1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcf2d4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import OxfordIIITPet\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70769d66",
   "metadata": {},
   "source": [
    "2. 定义超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52127e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "num_classes = 37  # Oxford-IIIT Pet Dataset has 37 categories\n",
    "image_size = 128  # 输入图像大小\n",
    "patch_size = 16  # 图像分割为16x16的patch\n",
    "num_patches = (image_size // patch_size) ** 2  # 每张图像的patch数量\n",
    "projection_dim = 64  # 投影维度\n",
    "num_heads = 4  # 注意力头数\n",
    "transformer_units = [projection_dim * 2, projection_dim]  # Transformer层的单位\n",
    "transformer_layers = 8  # Transformer层数\n",
    "mlp_head_units = [2048, 1024]  # MLP头的单位\n",
    "learning_rate = 0.001  # 学习率\n",
    "weight_decay = 0.0001 # 权重衰减\n",
    "batch_size = 32 # 批量大小\n",
    "num_epochs = 50 # 训练轮数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd6dfc2",
   "metadata": {},
   "source": [
    "3. 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 69.9M/792M [00:50<02:09, 5.58MB/s] "
     ]
    }
   ],
   "source": [
    "# Data preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = OxfordIIITPet(root='./data', split='trainval', target_types='category', download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = OxfordIIITPet(root='./data', split='test', target_types='category', download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a08b4b4",
   "metadata": {},
   "source": [
    "4. 定义MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795fd4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(units[0], units[1]))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7c652",
   "metadata": {},
   "source": [
    "5. 定义补丁嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ac9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Patch Embedding\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, embed_dim):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.projection = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.projection(x).flatten(2).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e7522",
   "metadata": {},
   "source": [
    "6. 定义Transformer块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_hidden_dim, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = MLP([[embed_dim, mlp_hidden_dim], [mlp_hidden_dim, embed_dim]], dropout)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output = self.attention(x, x, x)[0]\n",
    "        x = self.norm1(x + attention_output)\n",
    "        mlp_output = self.mlp(x)\n",
    "        x = self.norm2(x + mlp_output)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb672e7d",
   "metadata": {},
   "source": [
    "7. 定义Vision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b204e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Vision Transformer (ViT)\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, num_classes, image_size, patch_size, embed_dim, num_heads, transformer_units, num_layers, mlp_head_units):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.patch_embedding = PatchEmbedding(image_size, patch_size, embed_dim)\n",
    "        self.position_embedding = nn.Parameter(torch.zeros(1, self.patch_embedding.num_patches, embed_dim))\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, transformer_units[0], 0.1)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.mlp_head = MLP([[embed_dim, mlp_head_units[0]], [mlp_head_units[0], mlp_head_units[1]]], 0.5)\n",
    "        self.classifier = nn.Linear(mlp_head_units[1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x + self.position_embedding\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = x.mean(dim=1)\n",
    "        x = self.mlp_head(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb15ec2",
   "metadata": {},
   "source": [
    "8. 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447637d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = VisionTransformer(\n",
    "    num_classes=num_classes,\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    embed_dim=projection_dim,\n",
    "    num_heads=num_heads,\n",
    "    transformer_units=transformer_units,\n",
    "    num_layers=transformer_layers,\n",
    "    mlp_head_units=mlp_head_units\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(trainloader):\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        model = model.to('cuda')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216185f",
   "metadata": {},
   "source": [
    "9. 评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa049aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop with visualization\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    images, labels = next(iter(testloader))\n",
    "    images, labels = images.to('cuda'), labels.to('cuda')\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "    # Visualize predictions\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    for i in range(5):\n",
    "        img = images[i].permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5  # Unnormalize\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Predicted: {trainset.classes[predicted[i]]}\\nActual: {trainset.classes[labels[i]]}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d070a2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
